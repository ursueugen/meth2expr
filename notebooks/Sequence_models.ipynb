{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/nn_tutorial.html\n",
    "\n",
    "https://github.com/spro/practical-pytorch/tree/master/char-rnn-classification\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "\n",
    "Batch RNN\n",
    "https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdc9447c030>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "#print(torch.cuda.is_available())\n",
    "#if torch.cuda.is_available() or True:\n",
    "#    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "#    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "#    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "#    z = x + y\n",
    "#    print(z)\n",
    "#    print(z.to(\"cpu\", torch.double))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = ['A', 'C', 'T', 'G', 'N']\n",
    "BASE2IDX = {\"A\": 0, \"C\": 1, \"T\": 2, \"G\": 3, \"N\": 4}\n",
    "\n",
    "\n",
    "def findall(base: str, seq: str):\n",
    "    idxs = []\n",
    "    for m in re.finditer(base, seq):\n",
    "        idxs.append(m.start(0))\n",
    "    return idxs\n",
    "\n",
    "\n",
    "def random_seq(l: int, vocab: list = VOCAB) -> str:\n",
    "    return \"\".join(np.random.choice(vocab, l))\n",
    "\n",
    "\n",
    "def random_data(m: int, l: int, c: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Generates random dataset consisting of m sequences of length l with up to c cpg sites per sequence.\n",
    "    \"\"\"\n",
    "    assert c < l\n",
    "\n",
    "    seqs = [random_seq(l, ) for _ in range(m)]\n",
    "    cpgs = [[(np.random.choice(findall(\"C\", seqs[i])), np.random.random()) for _ in range(np.random.randint(0, c))] for i in range(m)]\n",
    "    exprs = [float(np.random.binomial(n=1, p=x.count('N')/len(x), size=1)[0]) for x in seqs]\n",
    "\n",
    "    return seqs, cpgs, exprs\n",
    "\n",
    "\n",
    "def base2tensor(base: str, vocab: list = VOCAB) -> torch.tensor:\n",
    "    tensor = torch.zeros(1, len(vocab))\n",
    "    tensor[BASE2IDX[base]][0] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def seq2tensor(seq: str, vocab: list = VOCAB) -> torch.tensor:\n",
    "    tensor = torch.zeros(len(seq), 1, len(vocab))  # important choice for preserving shape compatibility with hidden layer.\n",
    "    for idx, base in enumerate(seq):\n",
    "        tensor[idx][0][BASE2IDX[base]] = 1  # that extra 1 dimension is because PyTorch assumes everything is in batches - weâ€™re just using a batch size of 1 here.\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def methylate_seq(seq_tensor: torch.tensor, loc: int, value: float, meth_idx: int, mask_idx: int) -> torch.tensor:\n",
    "    assert (seq_tensor[loc][0][meth_idx] > 0) or (seq_tensor[loc][0][mask_idx] > 0)# assert it is a \"C\" in first place\n",
    "    seq_tensor[loc][0][meth_idx] = value\n",
    "    return seq_tensor\n",
    "\n",
    "\n",
    "class PromoterDataset(Dataset):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    def __init__(self, data: tuple):\n",
    "        \n",
    "        self.seqs, self.cpgs, self.exprs = data\n",
    "        \n",
    "        # get some utile attributes\n",
    "        self.vocab = sorted(set(\"\".join(self.seqs)))  # returns list\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.base_to_idx = {}\n",
    "        for i, base in enumerate(VOCAB):\n",
    "            self.base_to_idx[base] = i\n",
    "        \n",
    "        self.meth_base = \"C\"  #set methylated base\n",
    "        self.mask_base = \"N\"\n",
    "        self.meth_idx = self.base_to_idx[self.meth_base]\n",
    "        self.mask_idx = self.base_to_idx[self.mask_base]\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.exprs)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Getter.\"\"\"\n",
    "        seq = seq2tensor(self.seqs[idx])\n",
    "        cpgs = self.cpgs[idx]\n",
    "        expr = self.exprs[idx]\n",
    "\n",
    "        for cpg in cpgs:\n",
    "            # insert cpg value in row for meth_base (cytosine)\n",
    "            cpg_loc = cpg[0]\n",
    "            cpg_value = cpg[1]\n",
    "            seq = methylate_seq(seq, cpg_loc, cpg_value, self.meth_idx, self.mask_idx)\n",
    "\n",
    "        return seq, expr\n",
    "            \n",
    "\n",
    "m = 100\n",
    "l = 50\n",
    "c = 5\n",
    "VOCAB = ['A', 'C', 'T', 'G', 'N']\n",
    "BASE2IDX = {\"A\": 0, \"C\": 1, \"T\": 2, \"G\": 3, \"N\": 4}\n",
    "data = random_data(m, l, c)\n",
    "\n",
    "# test_loader, train_loader\n",
    "train_set = PromoterDataset(data)\n",
    "train_loader = DataLoader(train_set, batch_size=4, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNRegressor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(RNNRegressor, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.i2h = nn.Linear(input_dim + hidden_dim, hidden_dim)\n",
    "        self.i2o = nn.Linear(input_dim + hidden_dim, output_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        #input = input.view(-1, input.shape[1])\n",
    "        # TODO: Modify code to accept batch tensors <loc, batch_nr, index>. Can add init hidden in here.\n",
    "        \n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.tanh(output)\n",
    "\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_dim)\n",
    "\n",
    "HIDDEN_DIM = 10\n",
    "rnn = RNNRegressor(train_set.vocab_size, HIDDEN_DIM, 1)\n",
    "\n",
    "\n",
    "\n",
    "writer = SummaryWriter('runs/RNN_playground')\n",
    "writer.add_graph(rnn, (seq2tensor(\"ACGTT\")[1], torch.zeros(1, HIDDEN_DIM)))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 5]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "def train(output_tensor, seq_tensor):\n",
    "    ''''''\n",
    "    \n",
    "    hidden = rnn.initHidden()\n",
    "    \n",
    "    # zero the grad buffers\n",
    "    rnn.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # forward pass\n",
    "    for i in range(seq_tensor.shape[0]):\n",
    "        output, hidden = rnn(seq_tensor[i], hidden)\n",
    "    \n",
    "    # compute loss and backward pass\n",
    "    loss = criterion(output, output_tensor)\n",
    "    loss.backward()\n",
    "    \n",
    "    # update params\n",
    "    optimizer.step()\n",
    "    \n",
    "    return output, loss.item()\n",
    "\n",
    "train(torch.tensor([-0.0117]), seq2tensor(\"ACGTN\"))\n",
    "print(seq2tensor(\"ACGTN\").shape, torch.tensor([-0.0118]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "# TODO: not really minibatch for now\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        \n",
    "        # get inputs\n",
    "        # TODO: Account for dtypes, otherwise get incompatible!\n",
    "        seq_tensor, expr = data\n",
    "        seq_tensor = seq_tensor[0]\n",
    "        expr = expr[0].view(1)\n",
    "        expr = expr.type(torch.FloatTensor)  \n",
    "        \n",
    "        # train on example\n",
    "        output, loss = train(expr[0], seq_tensor)\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# save model\n",
    "PATH = './models/test_model'\n",
    "torch.save(rnn.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict loop\n",
    "\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# with torch.no_grad():\n",
    "#     for data in testloader:\n",
    "#         images, labels = data\n",
    "#         outputs = net(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "# print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "#     100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
