{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bitorthologconda2734be78404a403f81d7a020a58cc485",
   "display_name": "Python 3.7.5 64-bit ('ortholog': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7ff1f80730b0>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "#print(torch.cuda.is_available())\n",
    "#if torch.cuda.is_available() or True:\n",
    "#    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "#    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "#    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "#    z = x + y\n",
    "#    print(z)\n",
    "#    print(z.to(\"cpu\", torch.double))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_seq(l: int, vocab: list) -> str:\n",
    "    return \"\".join(np.random.choice(vocab, l))\n",
    "\n",
    "\n",
    "def random_data(m: int, l: int, c: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Generates random dataset consisting of m sequences of length l with up to c cpg sites per sequence.\n",
    "    \"\"\"\n",
    "    assert c < l\n",
    "\n",
    "    seqs = [random_seq() for _ in range(m)]\n",
    "    cpgs = [[(np.random.randint(0,l), np.random.random()) for _ in range(np.random.randint(0, c))] for _ in range(m)]\n",
    "    exprs = [float(np.random.binomial(n=1, p=x.count('N')/len(x), size=1)[0]) for x in seqs]\n",
    "\n",
    "    return seqs, cpgs, exprs\n",
    "\n",
    "\n",
    "def base2tensor(base: str) -> torch.tensor:\n",
    "    tensor = torch.zeros(1, VOCAB_SIZE)\n",
    "    tensor[0][BASE2IDX[base]] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def seq2tensor(seq: str) -> torch.tensor:\n",
    "    tensor = torch.zeros(len(seq), 1, VOCAB_SIZE)\n",
    "    for idx, base in enumerate(seq):\n",
    "        tensor[idx][0][BASE2IDX[base]] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def methylate_seq(seq_tensor: torch.tensor, loc: int, value: float, meth_idx: int, mask_idx: int) -> torch.tensor:\n",
    "    assert (seq_tensor[meth_idx, loc] == 1) or ()# assert it is a \"C\" in first place\n",
    "    seq_tensor[meth_idx, loc] = value\n",
    "    return seq_tensor\n",
    "\n",
    "\n",
    "class PromoterDataset(Dataset):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    def __init__(self, data: tuple):\n",
    "        \n",
    "        self.seqs, self.cpgs, self.exprs = data\n",
    "        \n",
    "        # get some utile attributes\n",
    "        self.vocab = sorted(set(\"\".join(self.seqs)))  # returns list\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.base_to_idx = {}\n",
    "        for i, base in enumerate(VOCAB):\n",
    "            base_to_idx[base] = i\n",
    "        \n",
    "        self.meth_base = \"C\"  #set methylated base\n",
    "        self.mask_base = \"N\"\n",
    "        self.meth_idx = self.base_to_idx[self.meth_base]\n",
    "        self.mask_idx = self.baset_to_idx[self.mask_base]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.exprs)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Getter.\"\"\"\n",
    "        seq = seq2tensor(self.seqs[idx])\n",
    "        cpgs = self.cpgs[idx]\n",
    "        expr = self.expr[idx]\n",
    "\n",
    "        for cpg in cpgs:\n",
    "            # insert cpg value in row for meth_base (cytosine)\n",
    "            cpg_loc = cpg[0]\n",
    "            cpg_value = cpg[1]\n",
    "            seq = methylate_seq(seq, loc, value, self.meth_idx, self.mask_idx)\n",
    "\n",
    "        return torch.tensor(seq, expr)\n",
    "            \n",
    "\n",
    "m = 100\n",
    "l = 50\n",
    "c = 5\n",
    "data = random_data(m, l, c)\n",
    "\n",
    "# test_loader, train_loader\n",
    "train_set = PromoterDataset(data)\n",
    "train_loader = DataLoader(train_set, batch_size=4, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNRegressor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(RNNRegressor, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.i2h = nn.Linear(input_dim + hidden_dim, hidden_dim)\n",
    "        self.i2o = nn.Linear(input_dim + hidden_dim, output_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.tanh(output)\n",
    "\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_dim)\n",
    "\n",
    "\n",
    "rnn = RNNRegressor(train_set.vocab_size, HIDDEN_DIM, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[-0.0117]], grad_fn=<TanhBackward>)\n"
    }
   ],
   "source": [
    "base2tensor(\"N\")\n",
    "seq2tensor(\"ACGTN\")\n",
    "\n",
    "input = seq2tensor('ACGTN')\n",
    "hidden = torch.zeros(1, HIDDEN_DIM)\n",
    "\n",
    "output, next_hidden = rnn(input[0], hidden)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[-0.0245]], grad_fn=<TanhBackward>), 0.0001632583880564198)"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "lr = 0.005\n",
    "\n",
    "def train(output_tensor, seq_tensor):\n",
    "\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(seq_tensor.size()[0]):\n",
    "        output, hidden = rnn(seq_tensor[i], hidden)\n",
    "    \n",
    "    loss = criterion(output, output_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-lr, p.grad.data)\n",
    "    \n",
    "    return output, loss.item()\n",
    "\n",
    "train(torch.tensor([-0.0117]), seq2tensor(\"ACGTN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.01711277849972248\n0.008758061565458775\n0.007915392518043518\n0.017463384196162224\n0.0056884572841227055\n0.0034697987139225006\n0.010249307379126549\n0.009265918284654617\n0.050086669623851776\n0.011109867133200169\n0.0008259211317636073\n0.0031845399644225836\n0.08377565443515778\n0.004317635670304298\n0.05308886244893074\n0.008003836497664452\n2.849230895662913e-07\n0.00017776194727048278\n0.0008829082362353802\n7.3491000875947066e-06\n0.011529708281159401\n0.00040584205999039114\n0.06075203791260719\n0.0008441368117928505\n0.009047843515872955\n0.00704139145091176\n0.010068253614008427\n3.899871444446035e-06\n0.004074526019394398\n0.0028674909844994545\n0.00014958357496652752\n1.00400972366333\n6.395148375304416e-07\n1.2135016918182373\n0.005960062611848116\n0.00030325434636324644\n0.0002610151714179665\n0.10282783955335617\n0.08487041294574738\n2.831971869454719e-05\n0.0010409540263935924\n0.001631260267458856\n0.005946222227066755\n0.00013461834168992937\n0.06925702095031738\n0.0022358756978064775\n0.000483204290503636\n0.0028181420639157295\n0.01719593070447445\n0.0743575245141983\n"
    }
   ],
   "source": [
    "n_iters = 50\n",
    "\n",
    "current_loss = 0\n",
    "for i in range(n_iters):\n",
    "    output_tensor, seq_tensor = torch.tensor(y_train[i]), seq2tensor(X_train[i])\n",
    "    output, loss = train(output_tensor, seq_tensor)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}